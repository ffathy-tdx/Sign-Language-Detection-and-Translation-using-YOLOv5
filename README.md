# Sign-Language-Detection-and-Translation-using-YOLOv5
**Overview:** This project is a sign language detection and translation model built using YOLOv5. The model first detects the user's hand using YOLOv5 and then enters the image into a CNN to translate it into the closest sign language image in our dataset. This project aims to provide a convenient and efficient way for users to communicate with those who use sign language, regardless of whether they know how to sign themselves.

**Features:**

Hand detection using YOLOv5: The model uses YOLOv5 to detect the user's hand in real-time.
Sign language image translation: Once the hand is detected, the model enters the image into a CNN to translate it into the closest sign language image in our dataset.
Customizable dataset: Users can add their own images to the dataset to improve the model's accuracy for specific signs.

**Examples and Use Cases:**

A user who does not know sign language can use this model to communicate with someone who does.
This model can also be used by people who are learning sign language to check if they are performing a sign correctly.

**Screenshots:**

![Image](https://user-images.githubusercontent.com/103205660/221358204-620a3595-b550-47ad-8e03-2a7338a34ffe.jpg)



![Image](https://user-images.githubusercontent.com/103205660/221358209-ecf2fdca-a885-4397-be50-3d65e2ed35bc.jpg)



![Image](https://user-images.githubusercontent.com/103205660/221358218-04febd58-a5dc-4433-b6d6-097539ccb7a3.jpg)

I appreciate your interest in my project!

hope this helps! Let me know if you have any further questions.
